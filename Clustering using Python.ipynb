{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9ccf48-012b-4d16-bb1e-9f0533e8fb15",
   "metadata": {},
   "source": [
    "# Clustering using Python tools\n",
    "Tested with QuPath 0.6.0rc3 on Win11<br>\n",
    "This section builds off of the following resources: https://www.youtube.com/watch?v=984Jz2QCvng<br>\n",
    "Instructions: https://github.com/qupath/i2k-qupath-for-python-programmers<br>\n",
    "Documentation?: https://qupath.github.io/qubalab-docs/<br>\n",
    "Thanks to the work of Alan O'Callaghan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd15a0d-3a16-4bef-8419-b13e76b2d213",
   "metadata": {},
   "source": [
    "Create an environment and install qubalab into it with<br>\n",
    "<kbd>conda create -n yourEnvironmentName python=3.10</kbd><br>\n",
    "<kbd>pip install --upgrade qubalab </kbd><br>\n",
    "<kbd>pip install ipython leidenalg igraph umap-learn</kbd><br>\n",
    "<kbd>pip install seaborn[stats]</kbd>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40afec-a96c-42cc-8e17-1e5774e2fbf0",
   "metadata": {},
   "source": [
    "## Start QuPath, open your project and an image, and open a connection\n",
    "The .gif shows both opening a connection, and seeing that the Python symbol is red, and closing the connection.<br>\n",
    "Do not close the connection unless you are done!<br>\n",
    "<img src= \"Images/ps_startconnection.gif\" width = \"800\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3643e36-d9ae-499b-b4cd-ce172a88fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubalab.qupath import qupath_gateway\n",
    "from qubalab.images.qupath_server import QuPathServer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#This uses the default gateway 25333, make sure to change this to match the one you created in QuPath if you altered this.\n",
    "gateway = qupath_gateway.create_gateway(auth_token=None, port=25333)\n",
    "gateway\n",
    "qupath_server = QuPathServer(gateway) # read pixels from qupath via gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98548810-2fe2-4840-9fe2-2272b49c9aeb",
   "metadata": {},
   "source": [
    "## Test the connection!\n",
    "Check the state of QuPath by taking a snapshot of the interface - not necessary, but something that is neat to know how to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e8404-45bd-4dd3-83df-611a39fa5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(qupath_gateway.create_snapshot())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e5695-b3dd-4161-bd59-dff0023df55c",
   "metadata": {},
   "source": [
    "# In QuPath\n",
    "I used the following script to create cells, feel free to follow along if you have access to the sample data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d69be-9571-48a0-84c2-671cdcbd837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "//QuPath script, do not run here!\n",
    "setImageType('FLUORESCENCE');\n",
    "createFullImageAnnotation(true)\n",
    "qupath.ext.instanseg.core.InstanSeg.builder()\n",
    "    .modelPath(\"F:/Scripts/fluorescence_nuclei_and_cells\")\n",
    "    .device(\"cpu\")\n",
    "    .inputChannels([ColorTransforms.createChannelExtractor(\"Hoechst\"), ColorTransforms.createChannelExtractor(\"CD31\"), ColorTransforms.createChannelExtractor(\"CD11c\"), ColorTransforms.createChannelExtractor(\"CD68\"), ColorTransforms.createChannelExtractor(\"CD163\"), ColorTransforms.createChannelExtractor(\"CD20\"), ColorTransforms.createChannelExtractor(\"CD4\"), ColorTransforms.createChannelExtractor(\"CD8a\"), ColorTransforms.createChannelExtractor(\"CD45RO\"), ColorTransforms.createChannelExtractor(\"PDL1\"), ColorTransforms.createChannelExtractor(\"LAG3\"), ColorTransforms.createChannelExtractor(\"PD1\"), ColorTransforms.createChannelExtractor(\"CD45\"), ColorTransforms.createChannelExtractor(\"S100a\")])\n",
    "    .outputChannels()\n",
    "    .tileDims(512)\n",
    "    .interTilePadding(32)\n",
    "    .nThreads(12)\n",
    "    .makeMeasurements(true)\n",
    "    .randomColors(false)\n",
    "    .build()\n",
    "    .detectObjects()\n",
    "clearSelectedObjects(true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dfd65-01b2-4adc-b59a-3588c4f5604c",
   "metadata": {},
   "source": [
    "## Data cleaning in QuPath\n",
    "Possibly run some filtering on the cells, to remove outliers or objects we do not think fit our analysis. These scripts are for use in QuPath, not Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc015bcf-3311-46eb-b188-6a0789d74c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "//QuPath script, do not run here\n",
    "\n",
    "toRemove = []\n",
    "//Cycle through all of the detected objects, and check for any that do not have a nucleus\n",
    "//There are a variety of ways of doing this, but one is checking for NaNs in measurements for nuclei.\n",
    "for (cell in getDetectionObjects()){\n",
    "    if (cell.measurements.get(\"Nucleus: Solidity\").isNaN()){\n",
    "        toRemove.add(cell)\n",
    "    }else if (cell.measurements.get(\"Nucleus: Area µm^2\") ==cell.measurements.get(\"Cell: Area µm^2\")){\n",
    "        //Additionally, remove any cells that have the exact same area of nucleus and cytoplasm.\n",
    "        //This is mostly a demonstration of another way to remove certain kinds of detected outliers.\n",
    "        toRemove.add(cell)\n",
    "    }\n",
    "}\n",
    "print \"Objects removed: \" + toRemove.size()\n",
    "removeObjects(toRemove, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630bfa0-5a1e-4f3d-9d11-239f917bfabb",
   "metadata": {},
   "source": [
    "Additionally, the UMAP function run below throws an error if ANY value is NaN. Since we are cycling through all of the \"mean\" values, let's go ahead and remove all cells where a measurement with **mean** in the name has a **NaN** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf142ec-ed29-473c-9b3d-6197e4475784",
   "metadata": {},
   "outputs": [],
   "source": [
    "//QuPath script do not run here\n",
    "\n",
    "toRemove = []\n",
    "\n",
    "// Iterate over all objects\n",
    "for (def pathObject in getAllObjects()) {\n",
    "    // Check if any measurement contains \"Mean\" and has a NaN value\n",
    "    if (pathObject.measurements.any { key, value -> key.contains(\"Mean\") && Double.isNaN(value) }) {\n",
    "        toRemove.add(pathObject)\n",
    "    }\n",
    "}\n",
    "\n",
    "// Remove the collected objects\n",
    "removeObjects(toRemove, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de1153-c87f-44dd-a60c-52c6c6afffcd",
   "metadata": {},
   "source": [
    "# Python\n",
    "## Create a dataframe using cell measurements\n",
    "Generate a basic dataframe using pandas, and normalize it - something important to keep in mind when performing clustering as different channels in a multiplex image can have vastly different values, yet represent the same amount of protein or have similar impacts on classification.<br>\n",
    "There are many methods of normalization, this is a fairly simple one.<br>\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4779207/<br>\n",
    "https://www.nature.com/articles/s41598-018-22489-1<br>\n",
    "Depending on the setting, variation in the samples (genetically identical mice vs genetically diverse humans, variation in human sample collection and fixation protocols) and other complications, different methods of normalization may be called for. Check with your local statistician and be ready to explain where sources of variation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f8833-b3dc-44cb-9a72-5e6aee11f4c7",
   "metadata": {},
   "source": [
    "### Note that the below method makes the very simple assumption that ALL images in the project are part of the project.\n",
    "Consider: Are the measurements in all channels relevant? Didn't we basically turn off some channels right at the start?<br>\n",
    "Further cleaning of your data can both improve results and reduce processing time!\n",
    "### Beyond this point, make sure to check whether you are running the \"Single image\" or \"Full project\" code - they do not mix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae1876-4b5f-475b-b087-b19d96bea86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full project\n",
    "\n",
    "import pandas as pd\n",
    "project = gateway.getProject()\n",
    "detections = []\n",
    "detection_to_image = {}\n",
    "\n",
    "for imageEntry in project.getImageList():\n",
    "    imageData = imageEntry.readImageData()\n",
    "    hierarchy = imageData.readHierarchy()\n",
    "    imageDetections = hierarchy.getDetectionObjects()\n",
    "    #Extra processing here to make sure we can map detections back to the correct image\n",
    "    for det in imageDetections:\n",
    "        uuid = det.getID().toString()\n",
    "        detections.append(det)\n",
    "        detection_to_image[uuid] = imageEntry.getID()  # Store mapping of detection to image number\n",
    "\n",
    "print(len(detections))\n",
    "names = detections[0].getMeasurementList().getNames()\n",
    "names = [x for x in names if x.endswith(\"Mean\")]\n",
    "\n",
    "df = pd.DataFrame(columns=[\"UUID\", \"ImageID\"] + names)\n",
    "for det in detections:\n",
    "    df = df._append(\n",
    "        {\n",
    "            \"UUID\": det.getID().toString(),\n",
    "            \"ImageID\": detection_to_image[det.getID().toString()],\n",
    "            **{name: det.getMeasurements().get(name) for name in names}\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Standardize numeric columns (excluding UUID and ImageID)\n",
    "numeric_df = df.drop(columns=[\"UUID\", \"ImageID\"])  \n",
    "normalized_df = (numeric_df - numeric_df.mean()) / numeric_df.std()\n",
    "\n",
    "#Check the state of your dataframes!\n",
    "#print(df.head())\n",
    "#print(normalized_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b474ff-20cb-45e8-8a7e-b37e30046cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image\n",
    "\n",
    "import pandas as pd\n",
    "detections = []\n",
    "detections = gateway.getDetectionObjects()\n",
    "names = detections[0].getMeasurementList().getNames()\n",
    "names = [x for x in names if x.endswith(\"Mean\")]\n",
    "\n",
    "df = pd.DataFrame(columns = names)\n",
    "for det in detections:\n",
    "    df = df._append(\n",
    "        {name: det.getMeasurements().get(name) for name in names},\n",
    "        ignore_index=True)\n",
    "\n",
    "\n",
    "# standardise the columns to avoid scale and shift effects\n",
    "normalized_df = (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d871412-f845-4b9b-abf8-18f9ca62ae19",
   "metadata": {},
   "source": [
    "## Running UMAP\n",
    "https://umap-learn.readthedocs.io/en/latest/<br>\n",
    "We could now run a dimensionality reduction algorithm, which might help us to visualise classifiers or other analyses in QuPath.<br>\n",
    "\n",
    "We can also add these features back into QuPath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85887d2-aa1c-4a19-8217-803c1c42bbbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full project\n",
    "import umap\n",
    "\n",
    "# Run UMAP dimensionality reduction\n",
    "embedding = umap.UMAP().fit_transform(normalized_df)\n",
    "\n",
    "# Create a dictionary mapping UUID to UMAP values\n",
    "umap_results = {df.iloc[i][\"UUID\"]: (float(embedding[i, 0]), float(embedding[i, 1])) for i in range(len(df))}\n",
    "\n",
    "for imageEntry in project.getImageList():\n",
    "    imageData = imageEntry.readImageData()\n",
    "    hierarchy = imageData.getHierarchy()\n",
    "    imageDetections = hierarchy.getDetectionObjects()\n",
    "    for det in imageDetections:\n",
    "        uuid = det.getID().toString()\n",
    "        image_id = imageEntry.getID()\n",
    "\n",
    "        # Ensure we're updating only detections that belong to the current image\n",
    "        if uuid in umap_results and detection_to_image[uuid] == image_id:\n",
    "            det.getMeasurementList().put(\"UMAP1\", umap_results[uuid][0])\n",
    "            det.getMeasurementList().put(\"UMAP2\", umap_results[uuid][1])\n",
    "    imageEntry.saveImageData(imageData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd75de3-ade9-4396-8401-100c70c8c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image\n",
    "import umap\n",
    "\n",
    "# run a dimensionality reduction algorithm on the measurements\n",
    "embedding = umap.UMAP().fit_transform(normalized_df)\n",
    "\n",
    "# assign back to measurement list\n",
    "for i in range(embedding.shape[0]):\n",
    "    detections[i].getMeasurementList().put(\"UMAP1\", float(embedding[i][0]))\n",
    "    detections[i].getMeasurementList().put(\"UMAP2\", float(embedding[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2088824-c1e3-4111-a74c-045b07402908",
   "metadata": {},
   "source": [
    "Plot the UMAP using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a0721-e4ab-4a78-93bb-1b2371201952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract UMAP1 and UMAP2 values from the embedding\n",
    "umap1 = embedding[:, 0]\n",
    "umap2 = embedding[:, 1]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(umap1, umap2, s=5, alpha=0.5)  # s=5 sets small point size, alpha=0.5 for transparency\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.title(\"UMAP Scatter Plot\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70434eb1-3b6c-44e2-bab3-c89437345421",
   "metadata": {},
   "source": [
    "Plot the UMAP using QuPath!<br>\n",
    "Run the following script in the QuPath script editor to plot the UMAP for the objects in the current image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41136185-7460-4437-9625-3556b3b2d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "//QuPath script, do not run here\n",
    "import qupath.lib.gui.charts.Charts\n",
    "\n",
    "nameOfActiveImage = getProjectEntry().getImageName()\n",
    "\n",
    "def builder = Charts.scatterChart()\n",
    "    .viewer(QuPathGUI.getInstance().getViewer())\n",
    "    .title('UMAP for '+ nameOfActiveImage)\n",
    "    .measurements(getDetectionObjects(), \"UMAP1\", \"UMAP2\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ba428-f76a-44fb-8c07-fa6a714eaf9a",
   "metadata": {},
   "source": [
    "Plot the UMAP for all detections in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ca309-fc32-4b68-bd5a-195e5f9b782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//QuPath script, do not run here\n",
    "import qupath.lib.gui.charts.Charts\n",
    "\n",
    "project = QP.getProject()\n",
    "detections = []\n",
    "for (imageEntry: project.getImageList()) {\n",
    "    hierarchy = imageEntry.readHierarchy()\n",
    "    detections += hierarchy.getDetectionObjects()\n",
    "}\n",
    "\n",
    "def builder = Charts.scatterChart()\n",
    "    .viewer(QuPathGUI.getInstance().getViewer())\n",
    "    .title('UMAP based on mean channel intensities')\n",
    "    .measurements(detections, \"UMAP1\", \"UMAP2\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae827f-5e5d-4fed-80ad-f5144fc5e7fe",
   "metadata": {},
   "source": [
    "## Preparing to cluster cells\n",
    "We can identify the KNN graph of the cells, and turn this into an adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b358251-40de-4034-a3c7-b7e7d32461b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "import igraph\n",
    "\n",
    "# find KNN graph\n",
    "A = kneighbors_graph(normalized_df, 50)\n",
    "\n",
    "# convert matrix to adjacency graph\n",
    "g = igraph.Graph.Adjacency((A > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411adfc-f5ae-4cdc-bc82-96466da751ed",
   "metadata": {},
   "source": [
    "### kneighbors_graph What’s happening?\n",
    "We are building a network (graph) where each cell (data point) is connected to its 50 nearest neighbors based on their feature similarity.<br><br>\n",
    "Why 50?<br>\n",
    "This number controls how connected the graph is. A small value (e.g., 5-10) might not connect all data points well, while a large value (e.g., 100+) could make clusters too mixed.<br>\n",
    "50 is a commonly used middle-ground: it provides enough connections to detect meaningful groups while keeping clusters distinct.<br><br>\n",
    "How does \"nearest neighbors\" work?<br>\n",
    "It measures similarity between cells based on their feature values (e.g., intensity measurements) and links each cell to its 50 most similar neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b1b50-2b10-4a53-8de5-8fbdff8648c2",
   "metadata": {},
   "source": [
    "### igraph.Graph.Adjacency What’s happening?\n",
    "The KNN graph (A) is stored as a matrix where each row represents a cell and each column represents whether it's connected to another cell.<br>\n",
    "The (A > 0) part checks whether there is a connection (1 for connected, 0 for not connected).<br>\n",
    "This converts the matrix into a proper graph structure using igraph, so that we can apply clustering.<br><br>\n",
    "Why use > 0?<br>\n",
    "This ensures that we only keep meaningful connections (i.e., cells that are actually linked in the KNN graph).<br>\n",
    "The matrix A stores distances between points, but all we care about is whether a connection exists.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9e9ad-fbf0-43ac-8e00-6df2c361aad3",
   "metadata": {},
   "source": [
    "## Running clustering\n",
    "Then, we can cluster the adjacency. We could also have done K-means or something similar.<br>\n",
    "https://leidenalg.readthedocs.io/en/stable/reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ae98b-9f40-4fae-85df-8f2025a2055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For project\n",
    "\n",
    "import leidenalg\n",
    "\n",
    "# partition the KNN graph based on graph modularity\n",
    "partition = leidenalg.find_partition(g, leidenalg.ModularityVertexPartition)\n",
    "\n",
    "# Create dictionary mapping UUIDs to cluster labels\n",
    "cluster_results = {\n",
    "    df.iloc[i][\"UUID\"]: f\"Cluster {partition.membership[i]}\" \n",
    "    for i in range(len(df))\n",
    "}\n",
    "\n",
    "# assign the partitions as classes to the original QuPath objects\n",
    "for imageEntry in project.getImageList():\n",
    "    imageData = imageEntry.readImageData()\n",
    "    hierarchy = imageData.getHierarchy()\n",
    "    imageDetections = hierarchy.getDetectionObjects()\n",
    "\n",
    "    for det in imageDetections:\n",
    "        uuid = det.getID().toString()\n",
    "        image_id = imageEntry.getID()\n",
    "\n",
    "        # Ensure we're updating only detections that belong to the current image\n",
    "        if uuid in cluster_results and detection_to_image[uuid] == image_id:\n",
    "            det.setClassification(cluster_results[uuid])\n",
    "    imageEntry.saveImageData(imageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79685a-d8b0-45cd-81ff-bf88c825e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For single image\n",
    "import leidenalg\n",
    "\n",
    "# partition the KNN graph based on graph modularity\n",
    "partition = leidenalg.find_partition(g, leidenalg.ModularityVertexPartition)\n",
    "\n",
    "# assign the partitions as classes to the original QuPath objects\n",
    "for i in range(embedding.shape[0]):\n",
    "    detections[i].setClassification(f\"Cluster {partition.membership[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908490d-6dc8-449b-bc7e-5e9b85c871fc",
   "metadata": {},
   "source": [
    "### find_partition What’s happening?\n",
    "\n",
    "This groups the cells into clusters using the Leiden algorithm, which is designed to find highly connected groups in a graph.<br>\n",
    "The method used here is ModularityVertexPartition, which finds groups that maximize a measure called modularity.<br><br>\n",
    "What is \"Modularity\"?<br>\n",
    "\n",
    "Modularity measures how well-defined clusters are.<br>\n",
    "A high modularity score means nodes in the same cluster are highly connected to each other, but weakly connected to nodes in other clusters.\n",
    "The algorithm keeps adjusting cluster assignments to maximize modularity, ensuring meaningful groups.<br><br>\n",
    "Why use ModularityVertexPartition?<br>\n",
    "\n",
    "This is a standard way of defining clusters in a network.<br>\n",
    "It ensures that groups are found based on connections, not just distance.<br>\n",
    "This method is commonly used in biological and medical data where we are dealing with complex, non-linear relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26525df-fc7e-43f3-a129-d6e8babc9c30",
   "metadata": {},
   "source": [
    "## What is wrong with clustering from one image?\n",
    "## How is classification different?\n",
    "## Create classifiers from clustering results???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87672d3-c20a-4bf2-bbb5-73d822aaf843",
   "metadata": {},
   "source": [
    "# Next we generate some heatmaps to inspect our clusters\n",
    "Clusters are great, but do we trust them? What do they mean? Are they just artifacts?<br>\n",
    "\n",
    "At this point there is no longer a differentiation between single and multiple image data sets, but you should be doing this with multiple images if possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d873e-224c-465a-8029-9b6a8cc469a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add cluster labels to the normalized DataFrame\n",
    "normalized_df[\"Cluster\"] = df[\"UUID\"].map(cluster_results)  # Assign clusters to data\n",
    "\n",
    "# Group by cluster and compute the mean of each **normalized** feature\n",
    "cluster_means = normalized_df.groupby(\"Cluster\").mean(numeric_only=True)\n",
    "\n",
    "# Plot heatmap with normalized values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cluster_means, cmap=\"coolwarm\", annot=False)  # Set annot=True to show values\n",
    "plt.title(\"Heatmap of Clustered Cells vs. Normalized Feature Measurements\")\n",
    "plt.xlabel(\"Feature Measurements (Normalized)\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dbf94-2865-432a-a278-794cc11730e9",
   "metadata": {},
   "source": [
    "## What a mess!\n",
    "Let's filter this, and rename some of the features so they do not clutter up our reading of the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957171b3-7c7e-40af-9860-2f3b8ea166a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set threshold for filtering features\n",
    "threshold = 2  # Ignore features with all values between -2 and 2\n",
    "\n",
    "# Add cluster labels to the normalized DataFrame\n",
    "normalized_df[\"Cluster\"] = df[\"UUID\"].map(cluster_results)  # Assign clusters to data\n",
    "\n",
    "# Group by cluster and compute the mean of each normalized feature\n",
    "cluster_means = normalized_df.groupby(\"Cluster\").mean(numeric_only=True)\n",
    "\n",
    "# Filter out features that do not exceed the threshold in any cluster\n",
    "filtered_features = cluster_means.columns[\n",
    "    (cluster_means.max(axis=0) > threshold) | (cluster_means.min(axis=0) < -threshold)\n",
    "]\n",
    "cluster_means_filtered = cluster_means[filtered_features]  # Keep only these features\n",
    "\n",
    "# Function to clean feature names\n",
    "def clean_feature_name(name):\n",
    "    name = name.replace(\": Mean\", \"\")  # Remove \": Mean\" suffix\n",
    "    name = name.replace(\"Nucleus\", \"N\").replace(\"Cytoplasm\", \"C\").replace(\"Membrane\", \"M\")  # Abbreviate\n",
    "    return name\n",
    "\n",
    "# Apply cleaning function to feature names\n",
    "cleaned_feature_names = [clean_feature_name(name) for name in cluster_means_filtered.columns]\n",
    "\n",
    "# Plot heatmap with filtered features\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.heatmap(cluster_means_filtered, cmap=\"coolwarm\", annot=False)  # Set annot=True to show values\n",
    "ax.set_xticklabels(cleaned_feature_names, rotation=45, ha=\"right\")  # Align right for better readability\n",
    "\n",
    "plt.title(\"Heatmap of Clustered Cells vs. Normalized Feature Measurements\")\n",
    "plt.xlabel(\"Feature Measurements (Filtered)\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ff419-bd9f-4acc-8530-9251b3e2bf14",
   "metadata": {},
   "source": [
    "## Patterns!\n",
    "\n",
    "The repeated patterns here suggest that we did not need all of the nuclear, cytoplasmic, and cell measurements, as the same channels show the same patterns for each cluster, per area of the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d4257-562d-4c5d-8bed-9137552a4fd3",
   "metadata": {},
   "source": [
    "What are the clusters without any extreme measurements? What changes with Cluster 9 if we set the threshold to 1.8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec71fa-64b2-4a79-974c-caa41e67c155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
